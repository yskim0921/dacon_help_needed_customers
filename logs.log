2025-10-17 09:40:51,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 09:40:51,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 09:40:51,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 09:40:51,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:04:01,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:04:01,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:04:01,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:04:01,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:06:55,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:06:55,001:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:06:55,001:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:06:55,001:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:08:01,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:08:01,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:08:01,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:08:01,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-17 10:08:06,330:INFO:PyCaret ClassificationExperiment
2025-10-17 10:08:06,330:INFO:Logging name: clf-default-name
2025-10-17 10:08:06,330:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-17 10:08:06,330:INFO:version 3.3.2
2025-10-17 10:08:06,331:INFO:Initializing setup()
2025-10-17 10:08:06,331:INFO:self.USI: 5ffa
2025-10-17 10:08:06,331:INFO:self._variable_keys: {'log_plots_param', 'n_jobs_param', 'memory', 'y_test', 'y', 'idx', 'exp_name_log', 'fold_groups_param', 'y_train', 'is_multiclass', 'exp_id', 'pipeline', '_available_plots', 'fix_imbalance', 'html_param', 'fold_generator', 'target_param', 'gpu_param', 'fold_shuffle_param', '_ml_usecase', 'seed', 'logging_param', 'USI', 'X_test', 'X', 'X_train', 'gpu_n_jobs_param', 'data'}
2025-10-17 10:08:06,331:INFO:Checking environment
2025-10-17 10:08:06,331:INFO:python_version: 3.10.18
2025-10-17 10:08:06,331:INFO:python_build: ('main', 'Jun  5 2025 13:08:55')
2025-10-17 10:08:06,331:INFO:machine: AMD64
2025-10-17 10:08:06,331:INFO:platform: Windows-10-10.0.22631-SP0
2025-10-17 10:08:06,343:INFO:Memory: svmem(total=8258007040, available=439529472, percent=94.7, used=7818477568, free=439529472)
2025-10-17 10:08:06,343:INFO:Physical Core: 10
2025-10-17 10:08:06,343:INFO:Logical Core: 12
2025-10-17 10:08:06,343:INFO:Checking libraries
2025-10-17 10:08:06,343:INFO:System:
2025-10-17 10:08:06,343:INFO:    python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
2025-10-17 10:08:06,344:INFO:executable: C:\Users\campus1N013\anaconda3\envs\alpaco_new\python.exe
2025-10-17 10:08:06,344:INFO:   machine: Windows-10-10.0.22631-SP0
2025-10-17 10:08:06,344:INFO:PyCaret required dependencies:
2025-10-17 10:08:08,131:INFO:                 pip: 25.2
2025-10-17 10:08:08,131:INFO:          setuptools: 80.9.0
2025-10-17 10:08:08,131:INFO:             pycaret: 3.3.2
2025-10-17 10:08:08,131:INFO:             IPython: 8.37.0
2025-10-17 10:08:08,131:INFO:          ipywidgets: 8.1.7
2025-10-17 10:08:08,131:INFO:                tqdm: 4.67.1
2025-10-17 10:08:08,131:INFO:               numpy: 1.26.4
2025-10-17 10:08:08,131:INFO:              pandas: 2.1.4
2025-10-17 10:08:08,131:INFO:              jinja2: 3.1.6
2025-10-17 10:08:08,131:INFO:               scipy: 1.11.4
2025-10-17 10:08:08,131:INFO:              joblib: 1.3.2
2025-10-17 10:08:08,131:INFO:             sklearn: 1.4.2
2025-10-17 10:08:08,131:INFO:                pyod: 2.0.5
2025-10-17 10:08:08,131:INFO:            imblearn: 0.14.0
2025-10-17 10:08:08,131:INFO:   category_encoders: 2.7.0
2025-10-17 10:08:08,131:INFO:            lightgbm: 4.6.0
2025-10-17 10:08:08,131:INFO:               numba: 0.61.0
2025-10-17 10:08:08,131:INFO:            requests: 2.32.5
2025-10-17 10:08:08,131:INFO:          matplotlib: 3.7.5
2025-10-17 10:08:08,131:INFO:          scikitplot: 0.3.7
2025-10-17 10:08:08,131:INFO:         yellowbrick: 1.5
2025-10-17 10:08:08,131:INFO:              plotly: 5.24.1
2025-10-17 10:08:08,131:INFO:    plotly-resampler: Not installed
2025-10-17 10:08:08,131:INFO:             kaleido: 1.1.0
2025-10-17 10:08:08,131:INFO:           schemdraw: 0.15
2025-10-17 10:08:08,131:INFO:         statsmodels: 0.14.5
2025-10-17 10:08:08,131:INFO:              sktime: 0.26.0
2025-10-17 10:08:08,131:INFO:               tbats: 1.1.3
2025-10-17 10:08:08,131:INFO:            pmdarima: 2.0.4
2025-10-17 10:08:08,131:INFO:              psutil: 7.1.0
2025-10-17 10:08:08,131:INFO:          markupsafe: 3.0.3
2025-10-17 10:08:08,131:INFO:             pickle5: Not installed
2025-10-17 10:08:08,131:INFO:         cloudpickle: 3.1.1
2025-10-17 10:08:08,131:INFO:         deprecation: 2.1.0
2025-10-17 10:08:08,131:INFO:              xxhash: 3.6.0
2025-10-17 10:08:08,132:INFO:           wurlitzer: Not installed
2025-10-17 10:08:08,132:INFO:PyCaret optional dependencies:
2025-10-17 10:08:14,531:INFO:                shap: 0.44.1
2025-10-17 10:08:14,531:INFO:           interpret: 0.7.3
2025-10-17 10:08:14,531:INFO:                umap: 0.5.7
2025-10-17 10:08:14,531:INFO:     ydata_profiling: 4.17.0
2025-10-17 10:08:14,531:INFO:  explainerdashboard: 0.5.1
2025-10-17 10:08:14,531:INFO:             autoviz: Not installed
2025-10-17 10:08:14,531:INFO:           fairlearn: 0.7.0
2025-10-17 10:08:14,531:INFO:          deepchecks: Not installed
2025-10-17 10:08:14,531:INFO:             xgboost: 3.0.5
2025-10-17 10:08:14,531:INFO:            catboost: 1.2.8
2025-10-17 10:08:14,531:INFO:              kmodes: 0.12.2
2025-10-17 10:08:14,531:INFO:             mlxtend: 0.23.4
2025-10-17 10:08:14,531:INFO:       statsforecast: 1.5.0
2025-10-17 10:08:14,531:INFO:        tune_sklearn: Not installed
2025-10-17 10:08:14,531:INFO:                 ray: Not installed
2025-10-17 10:08:14,531:INFO:            hyperopt: 0.2.7
2025-10-17 10:08:14,531:INFO:              optuna: 4.5.0
2025-10-17 10:08:14,531:INFO:               skopt: 0.10.2
2025-10-17 10:08:14,531:INFO:              mlflow: 3.5.0
2025-10-17 10:08:14,531:INFO:              gradio: 5.49.1
2025-10-17 10:08:14,531:INFO:             fastapi: 0.119.0
2025-10-17 10:08:14,531:INFO:             uvicorn: 0.37.0
2025-10-17 10:08:14,531:INFO:              m2cgen: 0.10.0
2025-10-17 10:08:14,531:INFO:           evidently: 0.4.40
2025-10-17 10:08:14,531:INFO:               fugue: 0.8.7
2025-10-17 10:08:14,531:INFO:           streamlit: Not installed
2025-10-17 10:08:14,531:INFO:             prophet: Not installed
2025-10-17 10:08:14,531:INFO:None
2025-10-17 10:08:14,532:INFO:Set up data.
2025-10-17 10:08:14,583:INFO:Set up folding strategy.
2025-10-17 10:08:14,583:INFO:Set up train/test split.
2025-10-17 10:08:14,613:INFO:Set up index.
2025-10-17 10:08:14,616:INFO:Assigning column types.
2025-10-17 10:08:14,623:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-17 10:08:14,650:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,653:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,680:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,681:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,712:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,713:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,732:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,734:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,735:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-17 10:08:14,765:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,786:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,788:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,817:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-17 10:08:14,836:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,838:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,839:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-17 10:08:14,888:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,890:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,945:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:14,947:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:14,951:INFO:Preparing preprocessing pipeline...
2025-10-17 10:08:14,954:INFO:Set up simple imputation.
2025-10-17 10:08:14,981:INFO:Finished creating preprocessing pipeline.
2025-10-17 10:08:14,986:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Public\Documents\ESTsoft\CreatorTemp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['age', 'gender', 'tenure',
                                             'frequent', 'payment_interval',
                                             'subscription_type',
                                             'contract_length',
                                             'after_interaction', 'cluster'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-10-17 10:08:14,986:INFO:Creating final display dataframe.
2025-10-17 10:08:15,071:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target     support_needs
2                   Target type        Multiclass
3           Original data shape       (30858, 10)
4        Transformed data shape       (30858, 10)
5   Transformed train set shape       (21600, 10)
6    Transformed test set shape        (9258, 10)
7              Numeric features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5ffa
2025-10-17 10:08:15,125:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:15,127:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:15,177:INFO:Soft dependency imported: xgboost: 3.0.5
2025-10-17 10:08:15,179:INFO:Soft dependency imported: catboost: 1.2.8
2025-10-17 10:08:15,180:INFO:setup() successfully completed in 8.86s...............
2025-10-17 10:08:15,180:INFO:Initializing compare_models()
2025-10-17 10:08:15,180:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-10-17 10:08:15,180:INFO:Checking exceptions
2025-10-17 10:08:15,187:INFO:Preparing display monitor
2025-10-17 10:08:15,193:INFO:Initializing Logistic Regression
2025-10-17 10:08:15,193:INFO:Total runtime is 0.0 minutes
2025-10-17 10:08:15,193:INFO:SubProcess create_model() called ==================================
2025-10-17 10:08:15,193:INFO:Initializing create_model()
2025-10-17 10:08:15,193:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:08:15,193:INFO:Checking exceptions
2025-10-17 10:08:15,193:INFO:Importing libraries
2025-10-17 10:08:15,193:INFO:Copying training dataset
2025-10-17 10:08:15,200:INFO:Defining folds
2025-10-17 10:08:15,200:INFO:Declaring metric variables
2025-10-17 10:08:15,200:INFO:Importing untrained model
2025-10-17 10:08:15,201:INFO:Logistic Regression Imported successfully
2025-10-17 10:08:15,201:INFO:Starting cross validation
2025-10-17 10:08:15,201:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:08:57,476:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,958:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:08:59,957:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:09:02,572:INFO:Calculating mean and std
2025-10-17 10:09:02,783:INFO:Creating metrics dataframe
2025-10-17 10:09:03,595:INFO:Uploading results into container
2025-10-17 10:09:03,649:INFO:Uploading model into container now
2025-10-17 10:09:03,808:INFO:_master_model_container: 1
2025-10-17 10:09:03,808:INFO:_display_container: 2
2025-10-17 10:09:03,913:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-17 10:09:03,917:INFO:create_model() successfully completed......................................
2025-10-17 10:09:21,367:INFO:SubProcess create_model() end ==================================
2025-10-17 10:09:21,391:INFO:Creating metrics dataframe
2025-10-17 10:09:21,421:INFO:Initializing K Neighbors Classifier
2025-10-17 10:09:21,422:INFO:Total runtime is 1.1038154125213624 minutes
2025-10-17 10:09:21,423:INFO:SubProcess create_model() called ==================================
2025-10-17 10:09:21,430:INFO:Initializing create_model()
2025-10-17 10:09:21,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:09:21,430:INFO:Checking exceptions
2025-10-17 10:09:21,430:INFO:Importing libraries
2025-10-17 10:09:21,432:INFO:Copying training dataset
2025-10-17 10:09:21,544:INFO:Defining folds
2025-10-17 10:09:21,544:INFO:Declaring metric variables
2025-10-17 10:09:21,545:INFO:Importing untrained model
2025-10-17 10:09:21,546:INFO:K Neighbors Classifier Imported successfully
2025-10-17 10:09:21,546:INFO:Starting cross validation
2025-10-17 10:09:21,549:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:09:56,434:INFO:Calculating mean and std
2025-10-17 10:09:56,460:INFO:Creating metrics dataframe
2025-10-17 10:09:56,528:INFO:Uploading results into container
2025-10-17 10:09:56,535:INFO:Uploading model into container now
2025-10-17 10:09:56,545:INFO:_master_model_container: 2
2025-10-17 10:09:56,545:INFO:_display_container: 2
2025-10-17 10:09:56,554:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-17 10:09:56,554:INFO:create_model() successfully completed......................................
2025-10-17 10:09:57,710:INFO:SubProcess create_model() end ==================================
2025-10-17 10:09:57,710:INFO:Creating metrics dataframe
2025-10-17 10:09:57,723:INFO:Initializing Naive Bayes
2025-10-17 10:09:57,723:INFO:Total runtime is 1.7088420192400615 minutes
2025-10-17 10:09:57,723:INFO:SubProcess create_model() called ==================================
2025-10-17 10:09:57,723:INFO:Initializing create_model()
2025-10-17 10:09:57,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:09:57,752:INFO:Checking exceptions
2025-10-17 10:09:57,752:INFO:Importing libraries
2025-10-17 10:09:57,753:INFO:Copying training dataset
2025-10-17 10:09:57,761:INFO:Defining folds
2025-10-17 10:09:57,761:INFO:Declaring metric variables
2025-10-17 10:09:57,761:INFO:Importing untrained model
2025-10-17 10:09:57,761:INFO:Naive Bayes Imported successfully
2025-10-17 10:09:57,763:INFO:Starting cross validation
2025-10-17 10:09:57,765:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:09:59,159:INFO:Calculating mean and std
2025-10-17 10:09:59,161:INFO:Creating metrics dataframe
2025-10-17 10:09:59,164:INFO:Uploading results into container
2025-10-17 10:09:59,165:INFO:Uploading model into container now
2025-10-17 10:09:59,166:INFO:_master_model_container: 3
2025-10-17 10:09:59,166:INFO:_display_container: 2
2025-10-17 10:09:59,166:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-17 10:09:59,166:INFO:create_model() successfully completed......................................
2025-10-17 10:09:59,346:INFO:SubProcess create_model() end ==================================
2025-10-17 10:09:59,347:INFO:Creating metrics dataframe
2025-10-17 10:09:59,352:INFO:Initializing Decision Tree Classifier
2025-10-17 10:09:59,352:INFO:Total runtime is 1.7359965284665426 minutes
2025-10-17 10:09:59,352:INFO:SubProcess create_model() called ==================================
2025-10-17 10:09:59,353:INFO:Initializing create_model()
2025-10-17 10:09:59,353:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:09:59,353:INFO:Checking exceptions
2025-10-17 10:09:59,353:INFO:Importing libraries
2025-10-17 10:09:59,353:INFO:Copying training dataset
2025-10-17 10:09:59,368:INFO:Defining folds
2025-10-17 10:09:59,368:INFO:Declaring metric variables
2025-10-17 10:09:59,369:INFO:Importing untrained model
2025-10-17 10:09:59,369:INFO:Decision Tree Classifier Imported successfully
2025-10-17 10:09:59,369:INFO:Starting cross validation
2025-10-17 10:09:59,370:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:09:59,898:INFO:Calculating mean and std
2025-10-17 10:09:59,900:INFO:Creating metrics dataframe
2025-10-17 10:09:59,903:INFO:Uploading results into container
2025-10-17 10:09:59,904:INFO:Uploading model into container now
2025-10-17 10:09:59,904:INFO:_master_model_container: 4
2025-10-17 10:09:59,904:INFO:_display_container: 2
2025-10-17 10:09:59,904:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-10-17 10:09:59,906:INFO:create_model() successfully completed......................................
2025-10-17 10:10:00,071:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:00,071:INFO:Creating metrics dataframe
2025-10-17 10:10:00,184:INFO:Initializing SVM - Linear Kernel
2025-10-17 10:10:00,184:INFO:Total runtime is 1.7498581449190775 minutes
2025-10-17 10:10:00,184:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:00,185:INFO:Initializing create_model()
2025-10-17 10:10:00,185:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:00,185:INFO:Checking exceptions
2025-10-17 10:10:00,185:INFO:Importing libraries
2025-10-17 10:10:00,185:INFO:Copying training dataset
2025-10-17 10:10:00,265:INFO:Defining folds
2025-10-17 10:10:00,265:INFO:Declaring metric variables
2025-10-17 10:10:00,265:INFO:Importing untrained model
2025-10-17 10:10:00,266:INFO:SVM - Linear Kernel Imported successfully
2025-10-17 10:10:00,266:INFO:Starting cross validation
2025-10-17 10:10:00,266:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:01,822:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:01,890:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:02,205:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:10:05,419:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,420:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,426:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,423:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,426:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,429:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,436:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,438:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,463:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:10:05,466:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:10:05,491:INFO:Calculating mean and std
2025-10-17 10:10:05,494:INFO:Creating metrics dataframe
2025-10-17 10:10:05,496:INFO:Uploading results into container
2025-10-17 10:10:05,496:INFO:Uploading model into container now
2025-10-17 10:10:05,496:INFO:_master_model_container: 5
2025-10-17 10:10:05,496:INFO:_display_container: 2
2025-10-17 10:10:05,497:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-17 10:10:05,498:INFO:create_model() successfully completed......................................
2025-10-17 10:10:05,645:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:05,646:INFO:Creating metrics dataframe
2025-10-17 10:10:05,649:INFO:Initializing Ridge Classifier
2025-10-17 10:10:05,649:INFO:Total runtime is 1.8409358700116476 minutes
2025-10-17 10:10:05,649:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:05,649:INFO:Initializing create_model()
2025-10-17 10:10:05,649:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:05,649:INFO:Checking exceptions
2025-10-17 10:10:05,649:INFO:Importing libraries
2025-10-17 10:10:05,650:INFO:Copying training dataset
2025-10-17 10:10:05,656:INFO:Defining folds
2025-10-17 10:10:05,657:INFO:Declaring metric variables
2025-10-17 10:10:05,657:INFO:Importing untrained model
2025-10-17 10:10:05,658:INFO:Ridge Classifier Imported successfully
2025-10-17 10:10:05,658:INFO:Starting cross validation
2025-10-17 10:10:05,659:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:05,737:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,764:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,767:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,770:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,773:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,788:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,792:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,789:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,794:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:05,833:INFO:Calculating mean and std
2025-10-17 10:10:05,835:INFO:Creating metrics dataframe
2025-10-17 10:10:05,838:INFO:Uploading results into container
2025-10-17 10:10:05,838:INFO:Uploading model into container now
2025-10-17 10:10:05,840:INFO:_master_model_container: 6
2025-10-17 10:10:05,840:INFO:_display_container: 2
2025-10-17 10:10:05,841:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-10-17 10:10:05,841:INFO:create_model() successfully completed......................................
2025-10-17 10:10:05,991:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:05,991:INFO:Creating metrics dataframe
2025-10-17 10:10:05,994:INFO:Initializing Random Forest Classifier
2025-10-17 10:10:05,994:INFO:Total runtime is 1.84668687582016 minutes
2025-10-17 10:10:05,994:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:05,995:INFO:Initializing create_model()
2025-10-17 10:10:05,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:05,995:INFO:Checking exceptions
2025-10-17 10:10:05,995:INFO:Importing libraries
2025-10-17 10:10:05,995:INFO:Copying training dataset
2025-10-17 10:10:06,002:INFO:Defining folds
2025-10-17 10:10:06,003:INFO:Declaring metric variables
2025-10-17 10:10:06,003:INFO:Importing untrained model
2025-10-17 10:10:06,004:INFO:Random Forest Classifier Imported successfully
2025-10-17 10:10:06,004:INFO:Starting cross validation
2025-10-17 10:10:06,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:16,860:INFO:Calculating mean and std
2025-10-17 10:10:16,888:INFO:Creating metrics dataframe
2025-10-17 10:10:16,898:INFO:Uploading results into container
2025-10-17 10:10:16,906:INFO:Uploading model into container now
2025-10-17 10:10:16,907:INFO:_master_model_container: 7
2025-10-17 10:10:16,908:INFO:_display_container: 2
2025-10-17 10:10:16,910:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-10-17 10:10:16,911:INFO:create_model() successfully completed......................................
2025-10-17 10:10:17,282:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:17,282:INFO:Creating metrics dataframe
2025-10-17 10:10:17,298:INFO:Initializing Quadratic Discriminant Analysis
2025-10-17 10:10:17,299:INFO:Total runtime is 2.0351030309995015 minutes
2025-10-17 10:10:17,299:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:17,299:INFO:Initializing create_model()
2025-10-17 10:10:17,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:17,299:INFO:Checking exceptions
2025-10-17 10:10:17,299:INFO:Importing libraries
2025-10-17 10:10:17,302:INFO:Copying training dataset
2025-10-17 10:10:17,323:INFO:Defining folds
2025-10-17 10:10:17,323:INFO:Declaring metric variables
2025-10-17 10:10:17,325:INFO:Importing untrained model
2025-10-17 10:10:17,326:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-17 10:10:17,326:INFO:Starting cross validation
2025-10-17 10:10:17,328:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:17,498:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,509:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,606:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,606:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,617:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,630:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,637:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,658:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,702:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,703:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:17,741:INFO:Calculating mean and std
2025-10-17 10:10:17,743:INFO:Creating metrics dataframe
2025-10-17 10:10:17,745:INFO:Uploading results into container
2025-10-17 10:10:17,746:INFO:Uploading model into container now
2025-10-17 10:10:17,747:INFO:_master_model_container: 8
2025-10-17 10:10:17,747:INFO:_display_container: 2
2025-10-17 10:10:17,747:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-17 10:10:17,747:INFO:create_model() successfully completed......................................
2025-10-17 10:10:17,996:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:17,996:INFO:Creating metrics dataframe
2025-10-17 10:10:18,001:INFO:Initializing Ada Boost Classifier
2025-10-17 10:10:18,001:INFO:Total runtime is 2.0468037088712054 minutes
2025-10-17 10:10:18,001:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:18,002:INFO:Initializing create_model()
2025-10-17 10:10:18,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:18,002:INFO:Checking exceptions
2025-10-17 10:10:18,002:INFO:Importing libraries
2025-10-17 10:10:18,002:INFO:Copying training dataset
2025-10-17 10:10:18,015:INFO:Defining folds
2025-10-17 10:10:18,015:INFO:Declaring metric variables
2025-10-17 10:10:18,016:INFO:Importing untrained model
2025-10-17 10:10:18,016:INFO:Ada Boost Classifier Imported successfully
2025-10-17 10:10:18,016:INFO:Starting cross validation
2025-10-17 10:10:18,017:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:18,131:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,136:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,152:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,165:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,170:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,170:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,182:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,184:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,191:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:18,192:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-17 10:10:19,580:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,595:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,687:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,690:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,708:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,730:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,744:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,781:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,788:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,793:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:19,827:INFO:Calculating mean and std
2025-10-17 10:10:19,830:INFO:Creating metrics dataframe
2025-10-17 10:10:19,834:INFO:Uploading results into container
2025-10-17 10:10:19,835:INFO:Uploading model into container now
2025-10-17 10:10:19,836:INFO:_master_model_container: 9
2025-10-17 10:10:19,836:INFO:_display_container: 2
2025-10-17 10:10:19,836:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-10-17 10:10:19,836:INFO:create_model() successfully completed......................................
2025-10-17 10:10:20,098:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:20,099:INFO:Creating metrics dataframe
2025-10-17 10:10:20,103:INFO:Initializing Gradient Boosting Classifier
2025-10-17 10:10:20,103:INFO:Total runtime is 2.0818476915359496 minutes
2025-10-17 10:10:20,104:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:20,104:INFO:Initializing create_model()
2025-10-17 10:10:20,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:20,104:INFO:Checking exceptions
2025-10-17 10:10:20,104:INFO:Importing libraries
2025-10-17 10:10:20,104:INFO:Copying training dataset
2025-10-17 10:10:20,117:INFO:Defining folds
2025-10-17 10:10:20,117:INFO:Declaring metric variables
2025-10-17 10:10:20,118:INFO:Importing untrained model
2025-10-17 10:10:20,118:INFO:Gradient Boosting Classifier Imported successfully
2025-10-17 10:10:20,118:INFO:Starting cross validation
2025-10-17 10:10:20,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:32,489:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:32,532:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,434:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,662:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,690:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,882:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,896:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:33,948:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:34,396:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:34,703:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:34,728:INFO:Calculating mean and std
2025-10-17 10:10:34,730:INFO:Creating metrics dataframe
2025-10-17 10:10:34,732:INFO:Uploading results into container
2025-10-17 10:10:34,732:INFO:Uploading model into container now
2025-10-17 10:10:34,733:INFO:_master_model_container: 10
2025-10-17 10:10:34,733:INFO:_display_container: 2
2025-10-17 10:10:34,978:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-17 10:10:34,978:INFO:create_model() successfully completed......................................
2025-10-17 10:10:35,140:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:35,140:INFO:Creating metrics dataframe
2025-10-17 10:10:35,143:INFO:Initializing Linear Discriminant Analysis
2025-10-17 10:10:35,143:INFO:Total runtime is 2.332508889834086 minutes
2025-10-17 10:10:35,144:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:35,144:INFO:Initializing create_model()
2025-10-17 10:10:35,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:35,144:INFO:Checking exceptions
2025-10-17 10:10:35,144:INFO:Importing libraries
2025-10-17 10:10:35,144:INFO:Copying training dataset
2025-10-17 10:10:35,150:INFO:Defining folds
2025-10-17 10:10:35,150:INFO:Declaring metric variables
2025-10-17 10:10:35,151:INFO:Importing untrained model
2025-10-17 10:10:35,151:INFO:Linear Discriminant Analysis Imported successfully
2025-10-17 10:10:35,151:INFO:Starting cross validation
2025-10-17 10:10:35,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:10:37,336:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,337:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,338:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,338:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,338:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,339:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,341:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,350:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,384:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,402:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-17 10:10:37,426:INFO:Calculating mean and std
2025-10-17 10:10:37,429:INFO:Creating metrics dataframe
2025-10-17 10:10:37,432:INFO:Uploading results into container
2025-10-17 10:10:37,433:INFO:Uploading model into container now
2025-10-17 10:10:37,433:INFO:_master_model_container: 11
2025-10-17 10:10:37,434:INFO:_display_container: 2
2025-10-17 10:10:37,434:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-17 10:10:37,434:INFO:create_model() successfully completed......................................
2025-10-17 10:10:37,686:INFO:SubProcess create_model() end ==================================
2025-10-17 10:10:37,686:INFO:Creating metrics dataframe
2025-10-17 10:10:37,689:INFO:Initializing Extra Trees Classifier
2025-10-17 10:10:37,689:INFO:Total runtime is 2.37494779030482 minutes
2025-10-17 10:10:37,690:INFO:SubProcess create_model() called ==================================
2025-10-17 10:10:37,690:INFO:Initializing create_model()
2025-10-17 10:10:37,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:10:37,690:INFO:Checking exceptions
2025-10-17 10:10:37,690:INFO:Importing libraries
2025-10-17 10:10:37,690:INFO:Copying training dataset
2025-10-17 10:10:37,703:INFO:Defining folds
2025-10-17 10:10:37,704:INFO:Declaring metric variables
2025-10-17 10:10:37,704:INFO:Importing untrained model
2025-10-17 10:10:37,704:INFO:Extra Trees Classifier Imported successfully
2025-10-17 10:10:37,704:INFO:Starting cross validation
2025-10-17 10:10:37,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:11:47,136:INFO:Calculating mean and std
2025-10-17 10:11:47,211:INFO:Creating metrics dataframe
2025-10-17 10:11:47,347:INFO:Uploading results into container
2025-10-17 10:11:47,366:INFO:Uploading model into container now
2025-10-17 10:11:47,388:INFO:_master_model_container: 12
2025-10-17 10:11:47,388:INFO:_display_container: 2
2025-10-17 10:11:47,412:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-10-17 10:11:47,413:INFO:create_model() successfully completed......................................
2025-10-17 10:11:48,980:INFO:SubProcess create_model() end ==================================
2025-10-17 10:11:48,980:INFO:Creating metrics dataframe
2025-10-17 10:11:48,985:INFO:Initializing Extreme Gradient Boosting
2025-10-17 10:11:48,985:INFO:Total runtime is 3.5632048487663273 minutes
2025-10-17 10:11:48,985:INFO:SubProcess create_model() called ==================================
2025-10-17 10:11:48,986:INFO:Initializing create_model()
2025-10-17 10:11:48,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:11:48,986:INFO:Checking exceptions
2025-10-17 10:11:48,987:INFO:Importing libraries
2025-10-17 10:11:48,988:INFO:Copying training dataset
2025-10-17 10:11:49,017:INFO:Defining folds
2025-10-17 10:11:49,017:INFO:Declaring metric variables
2025-10-17 10:11:49,017:INFO:Importing untrained model
2025-10-17 10:11:49,019:INFO:Extreme Gradient Boosting Imported successfully
2025-10-17 10:11:49,019:INFO:Starting cross validation
2025-10-17 10:11:49,020:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:11:50,686:INFO:Calculating mean and std
2025-10-17 10:11:50,688:INFO:Creating metrics dataframe
2025-10-17 10:11:50,691:INFO:Uploading results into container
2025-10-17 10:11:50,692:INFO:Uploading model into container now
2025-10-17 10:11:50,692:INFO:_master_model_container: 13
2025-10-17 10:11:50,692:INFO:_display_container: 2
2025-10-17 10:11:50,696:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-10-17 10:11:50,697:INFO:create_model() successfully completed......................................
2025-10-17 10:11:50,878:INFO:SubProcess create_model() end ==================================
2025-10-17 10:11:50,879:INFO:Creating metrics dataframe
2025-10-17 10:11:50,885:INFO:Initializing Light Gradient Boosting Machine
2025-10-17 10:11:50,886:INFO:Total runtime is 3.594883481661479 minutes
2025-10-17 10:11:50,886:INFO:SubProcess create_model() called ==================================
2025-10-17 10:11:50,886:INFO:Initializing create_model()
2025-10-17 10:11:50,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:11:50,886:INFO:Checking exceptions
2025-10-17 10:11:50,886:INFO:Importing libraries
2025-10-17 10:11:50,886:INFO:Copying training dataset
2025-10-17 10:11:50,897:INFO:Defining folds
2025-10-17 10:11:50,897:INFO:Declaring metric variables
2025-10-17 10:11:50,897:INFO:Importing untrained model
2025-10-17 10:11:50,898:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-17 10:11:50,898:INFO:Starting cross validation
2025-10-17 10:11:50,899:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:11:55,949:INFO:Calculating mean and std
2025-10-17 10:11:55,951:INFO:Creating metrics dataframe
2025-10-17 10:11:55,954:INFO:Uploading results into container
2025-10-17 10:11:55,955:INFO:Uploading model into container now
2025-10-17 10:11:55,956:INFO:_master_model_container: 14
2025-10-17 10:11:55,956:INFO:_display_container: 2
2025-10-17 10:11:55,957:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-17 10:11:55,958:INFO:create_model() successfully completed......................................
2025-10-17 10:11:56,196:INFO:SubProcess create_model() end ==================================
2025-10-17 10:11:56,196:INFO:Creating metrics dataframe
2025-10-17 10:11:56,206:INFO:Initializing CatBoost Classifier
2025-10-17 10:11:56,206:INFO:Total runtime is 3.683551712830862 minutes
2025-10-17 10:11:56,207:INFO:SubProcess create_model() called ==================================
2025-10-17 10:11:56,207:INFO:Initializing create_model()
2025-10-17 10:11:56,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:11:56,207:INFO:Checking exceptions
2025-10-17 10:11:56,207:INFO:Importing libraries
2025-10-17 10:11:56,207:INFO:Copying training dataset
2025-10-17 10:11:56,220:INFO:Defining folds
2025-10-17 10:11:56,221:INFO:Declaring metric variables
2025-10-17 10:11:56,221:INFO:Importing untrained model
2025-10-17 10:11:56,233:INFO:CatBoost Classifier Imported successfully
2025-10-17 10:11:56,233:INFO:Starting cross validation
2025-10-17 10:11:56,235:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:12:11,599:INFO:Calculating mean and std
2025-10-17 10:12:11,603:INFO:Creating metrics dataframe
2025-10-17 10:12:11,606:INFO:Uploading results into container
2025-10-17 10:12:11,607:INFO:Uploading model into container now
2025-10-17 10:12:11,607:INFO:_master_model_container: 15
2025-10-17 10:12:11,607:INFO:_display_container: 2
2025-10-17 10:12:11,608:INFO:<catboost.core.CatBoostClassifier object at 0x0000018E79EA54B0>
2025-10-17 10:12:11,608:INFO:create_model() successfully completed......................................
2025-10-17 10:12:11,884:WARNING:create_model() for <catboost.core.CatBoostClassifier object at 0x0000018E79EA54B0> raised an exception or returned all 0.0, trying without fit_kwargs:
2025-10-17 10:12:12,085:WARNING:Traceback (most recent call last):
  File "C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-10-17 10:12:12,085:INFO:Initializing create_model()
2025-10-17 10:12:12,085:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:12:12,085:INFO:Checking exceptions
2025-10-17 10:12:12,085:INFO:Importing libraries
2025-10-17 10:12:12,085:INFO:Copying training dataset
2025-10-17 10:12:12,094:INFO:Defining folds
2025-10-17 10:12:12,094:INFO:Declaring metric variables
2025-10-17 10:12:12,094:INFO:Importing untrained model
2025-10-17 10:12:12,094:INFO:CatBoost Classifier Imported successfully
2025-10-17 10:12:12,095:INFO:Starting cross validation
2025-10-17 10:12:12,095:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:12:48,307:INFO:Calculating mean and std
2025-10-17 10:12:48,321:INFO:Creating metrics dataframe
2025-10-17 10:12:48,362:INFO:Uploading results into container
2025-10-17 10:12:48,365:INFO:Uploading model into container now
2025-10-17 10:12:48,372:INFO:_master_model_container: 16
2025-10-17 10:12:48,372:INFO:_display_container: 2
2025-10-17 10:12:48,372:INFO:<catboost.core.CatBoostClassifier object at 0x0000018E79EA4850>
2025-10-17 10:12:48,373:INFO:create_model() successfully completed......................................
2025-10-17 10:12:49,539:INFO:SubProcess create_model() end ==================================
2025-10-17 10:12:49,539:INFO:Creating metrics dataframe
2025-10-17 10:12:49,557:INFO:Initializing Dummy Classifier
2025-10-17 10:12:49,557:INFO:Total runtime is 4.572739923000336 minutes
2025-10-17 10:12:49,558:INFO:SubProcess create_model() called ==================================
2025-10-17 10:12:49,558:INFO:Initializing create_model()
2025-10-17 10:12:49,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018E7A0BCDF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:12:49,559:INFO:Checking exceptions
2025-10-17 10:12:49,559:INFO:Importing libraries
2025-10-17 10:12:49,559:INFO:Copying training dataset
2025-10-17 10:12:49,582:INFO:Defining folds
2025-10-17 10:12:49,582:INFO:Declaring metric variables
2025-10-17 10:12:49,583:INFO:Importing untrained model
2025-10-17 10:12:49,583:INFO:Dummy Classifier Imported successfully
2025-10-17 10:12:49,584:INFO:Starting cross validation
2025-10-17 10:12:49,586:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-17 10:12:49,945:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,946:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,964:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,974:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,980:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,992:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:49,996:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:50,043:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:50,320:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:50,415:WARNING:C:\Users\campus1N013\anaconda3\envs\alpaco_new\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-17 10:12:51,246:INFO:Calculating mean and std
2025-10-17 10:12:51,248:INFO:Creating metrics dataframe
2025-10-17 10:12:51,249:INFO:Uploading results into container
2025-10-17 10:12:51,250:INFO:Uploading model into container now
2025-10-17 10:12:51,250:INFO:_master_model_container: 17
2025-10-17 10:12:51,250:INFO:_display_container: 2
2025-10-17 10:12:51,273:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-10-17 10:12:51,274:INFO:create_model() successfully completed......................................
2025-10-17 10:12:51,530:INFO:SubProcess create_model() end ==================================
2025-10-17 10:12:51,530:INFO:Creating metrics dataframe
2025-10-17 10:12:51,544:INFO:Initializing create_model()
2025-10-17 10:12:51,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:12:51,544:INFO:Checking exceptions
2025-10-17 10:12:51,549:INFO:Importing libraries
2025-10-17 10:12:51,549:INFO:Copying training dataset
2025-10-17 10:12:51,570:INFO:Defining folds
2025-10-17 10:12:51,570:INFO:Declaring metric variables
2025-10-17 10:12:51,570:INFO:Importing untrained model
2025-10-17 10:12:51,570:INFO:Declaring custom model
2025-10-17 10:12:51,571:INFO:Gradient Boosting Classifier Imported successfully
2025-10-17 10:12:51,574:INFO:Cross validation set to False
2025-10-17 10:12:51,574:INFO:Fitting Model
2025-10-17 10:13:00,238:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-17 10:13:00,238:INFO:create_model() successfully completed......................................
2025-10-17 10:13:00,476:INFO:Initializing create_model()
2025-10-17 10:13:00,477:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:13:00,477:INFO:Checking exceptions
2025-10-17 10:13:00,478:INFO:Importing libraries
2025-10-17 10:13:00,478:INFO:Copying training dataset
2025-10-17 10:13:00,487:INFO:Defining folds
2025-10-17 10:13:00,487:INFO:Declaring metric variables
2025-10-17 10:13:00,487:INFO:Importing untrained model
2025-10-17 10:13:00,487:INFO:Declaring custom model
2025-10-17 10:13:00,488:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-17 10:13:00,489:INFO:Cross validation set to False
2025-10-17 10:13:00,489:INFO:Fitting Model
2025-10-17 10:13:00,651:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.
2025-10-17 10:13:00,651:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-17 10:13:00,651:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-17 10:13:00,655:INFO:[LightGBM] [Info] Total Bins 215
2025-10-17 10:13:00,658:INFO:[LightGBM] [Info] Number of data points in the train set: 21600, number of used features: 9
2025-10-17 10:13:00,661:INFO:[LightGBM] [Info] Start training from score -0.769408
2025-10-17 10:13:00,661:INFO:[LightGBM] [Info] Start training from score -1.313457
2025-10-17 10:13:00,661:INFO:[LightGBM] [Info] Start training from score -1.317425
2025-10-17 10:13:01,520:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-17 10:13:01,520:INFO:create_model() successfully completed......................................
2025-10-17 10:13:01,791:INFO:Initializing create_model()
2025-10-17 10:13:01,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018E5C024DF0>, estimator=<catboost.core.CatBoostClassifier object at 0x0000018E79EA4850>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-17 10:13:01,792:INFO:Checking exceptions
2025-10-17 10:13:01,793:INFO:Importing libraries
2025-10-17 10:13:01,793:INFO:Copying training dataset
2025-10-17 10:13:01,817:INFO:Defining folds
2025-10-17 10:13:01,818:INFO:Declaring metric variables
2025-10-17 10:13:01,818:INFO:Importing untrained model
2025-10-17 10:13:01,818:INFO:Declaring custom model
2025-10-17 10:13:01,819:INFO:CatBoost Classifier Imported successfully
2025-10-17 10:13:01,821:INFO:Cross validation set to False
2025-10-17 10:13:01,821:INFO:Fitting Model
2025-10-17 10:13:10,362:INFO:<catboost.core.CatBoostClassifier object at 0x0000018E79EA5C30>
2025-10-17 10:13:10,363:INFO:create_model() successfully completed......................................
2025-10-17 10:13:11,497:INFO:_master_model_container: 17
2025-10-17 10:13:11,497:INFO:_display_container: 2
2025-10-17 10:13:11,498:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), <catboost.core.CatBoostClassifier object at 0x0000018E79EA5C30>]
2025-10-17 10:13:11,498:INFO:compare_models() successfully completed......................................
